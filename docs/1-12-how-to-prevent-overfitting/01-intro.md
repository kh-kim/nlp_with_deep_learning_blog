---
layout: default
title: How to Evaluate
parent: How to Prevent Overfitting
nav_order: 1-12-1
---

# 모델 평가하기

이제까지 이전 챕터에서 우리는 어떻게 모델을 잘 학습시킬 것인지 배웠습니다.
이제는 이렇게 학습시킨 모델이 얼마나 잘 $f^*$ 를 잘 근사<sup>approximate</sup>하고 있는지 평가하는 방법에 대해서 이야기해보고자 합니다.

다음 도식은 이미 여러번 설명한 머신러닝 프로젝트를 수행하는 절차입니다.

![](../../assets/images/1-12/01-workflow.png)

앞선 챕터들의 내용들은 대부분 알고리즘 적용 단계에 해당된다고 볼 수 있습니다.
이제 그럼 우리가 학습한 모델이 실제 우리가 원하는대로 잘 동작하는지 평가해야하는 다음 단계가 남아있습니다.
일부 독자들은 이미 그것은 손실 함수를 통해 우리가 배웠고 수행한 것이 아니냐고 반문하실 수 있지만, 사실은 알고보면 이제까지 우리가 해 온 방법은 제대로 된 방법이 아니었습니다.

기존에 우리는 $f^*$ 를 근사하기 위해서, 입력을 넣었을 때 원하는 출력이 나오도록 손실 함수를 최소화하는 입력 파라미터를 찾고자 하였습니다.
즉, 우리의 목표는 학습하는 데이터셋에 대한 손실 값을 최소화하는 것이었습니다.
하지만 이 방향은 우리의 모델이 수집된 데이터에만 잘 동작하도록 하는 현상인 오버피팅<sup>overfitting</sup>에 빠질 수 있습니다.
따라서 이번 챕터에서는 수집된 데이터셋을 잘 학습하면서도 오버피팅을 피할 수 있도록하는 방법에 대해서 소개하고자합니다.
그렇다면, 그에 앞서 우리의 모델이 객관적으로 잘 동작하는지 평가할 수 있어야 할 것입니다.

## 좋은 평가란

우리는 결국 학습이 완료된 모델 $f_\theta$ 를 실제 서비스에 배포하는 과정을 거치게 될 것입니다.
그리고 우리는 그 모델이 서비스 상에서 계속 잘 동작하기를 기대하겠지요.
따라서 좋은 평가란 여러가지 정의가 가능하겠지만, 실제 서비스에서 들어올 데이터와 최대한 비슷한 데이터를 활용한 평가가 될 수 있습니다.

그런데 만약 실제 서비스에서 들어오는 데이터가 학습된 데이터와 다른 성질을 갖는다면, 우리의 모델은 아마 잘 동작하지 않을 가능성이 높습니다.
이런 경우에는 우리가 학습 데이터가 편향되었거나, 잘못 수집되었다고 볼 수 있겠지요.
또는 실제 서비스에서 들어오는 데이터가 학습 데이터와 비슷한 성질을 갖더라도, 모델의 학습 자체가 잘 되지 않아서 애초에 잘 동작하지 않는 모델일 가능성도 있습니다.
이 경우에는 학습 방식 또는 모델 구조를 개선하여, 좀 더 모델이 데이터로부터 관계를 잘 학습할 수 있도록 해야 할 것입니다.
따라서 모델을 실제 배포하고나서 이런 상황을 발견하는 것보다, 미리 잘 준비된 테스트 데이터셋을 통해 객관적인 평가를 수행하는 것이 중요합니다.

테스트를 위한 데이터셋의 난이도도 중요한 포인트 중에 하나입니다.
사실 실제 서비스에서 모델이 예측할 데이터는 대부분 쉬운 수준의 샘플들일 것입니다.
아마 대부분은 쉬운 가운데, 그 중에서 가끔 어려운 데이터가 주어질 수 있지요.
그런데 서비스의 품질을 판가름 하는 것은 바로 그 어려운 데이터가 될 것입니다.
사람들이 느끼는 서비스의 품질은 아마 그 부분에서 판가름 나겠지요.
시험과 마찬가지입니다.
시험 문제에서 결국 우수한 학생과 그렇지 않은 학생을 구분하는 것은 소수의 높은 난이도의 문제일테니까요.

그러므로 우리는 테스트 데이터셋을 공들여 구성해야 할 필요성이 있습니다.
우리는 결국 이 평가를 통해서 타사의 서비스 또는 실험 내에서의 베이스라인<sup>baseline</sup>과 비교를 수행하게 될 것입니다.
이때, 너무 문제가 쉽다면 경쟁 모델과 나의 모델이 모두 높은 점수를 받을 것이고, 너무 문제가 어렵다면 경쟁 모델과 나의 모델이 모두 낮은 점수를 받을 것입니다.
따라서 변별력이 떨어지겠지요.

그래서 보통 테스트 데이터셋은 단순히 수집된 데이터셋들 중에서 임의로 고르기보단, 적절한 수준의 샘플들을 직접 엄선하여 구성하게 됩니다.
데이터셋은 가끔 레이블<sup>label</sup>이 틀린 샘플도 있으며, 대부분 굉장히 전형적<sup>typical</sup>인 난이도로 구성되어 있기 때문입니다.

## 정성평가와 정량평가

학습이 완료된 모델이 테스트 데이터셋에 대헤서 추론<sup>inference</sup>을 수행하고 나면, 예측된 값<sup>predicted value</sup>을 실제 정답과 비교함으로써 모델의 성능을 알 수 있을 것입니다.
이때, 분류<sup>classification</sup>와 같이 명확하게 채점이 가능한 경우도 있지만, 이미지 생성<sup>image generation</sup>이나 문장 생성<sup>sentence generation</sup>(e.g. 기계번역<sup>machine translation</sup>)과 같이 채점이 애매한 문제들도 있습니다.

따라서 채점의 명확한 기준이 없거나 정답이 정해져 있지 않은 경우에는 정량평가<sup>intrinsic evaluation</sup>를 수행하는 것이 가장 정확합니다.
정량평가란 실제 사람이 예측된 결과 값을 채점하는 것인데요.
예를 들어 한영 기계 번역 문제의 경우에, 입력 한국어 문장을 보고 예측된 영어 문장에 대해서 번역 정확도를 채점하는 것입니다.
당연히 사람이 평가를 진행하는 만큼 가장 정확하다고 볼 수 있으나, 사람에 의한 편차가 존재하기도 합니다.
또한 사람이 평가하는 속도는 한정되어 있기 때문에, 평가 비용이 비싸고 소요 시간도 오래걸리는 단점이 존재합니다.

이러한 정량평가의 단점을 보완하기 위해 정성평가<sup>extrinsic evaluation</sup>도 보통 함께 진행됩니다.
잘 고안된 점수 채점 공식<sup>score metric</sup>에 의해서 테스트 데이터셋 예측 결과 값에 대한 채점을 진행할 수 있는데요.
당연히 컴퓨터가 자동으로 채점을 진행하는 만큼, 대량 평가 및 자동화가 가능하며 비용과 시간 측면에서 이점이 있습니다.
물론 채점 공식이 사람의 채점 기준을 완벽하게 반영하지 못할 수 있기 때문에, 정성평가와 약간의 괴리가 존재할 수 있고, 이러한 괴리를 줄이기위한 채점 공식에 대한 연구들도 종종 진행됩니다.

|방법|장점|특징|
|-|-|-|
|정성평가<sup>intrinsic evaluation</sup>|정확함|사람에 의한 채점 방식. 채점 기준이 모호한 경우 필수.|
|정량평가<sup>extrinsic evaluation</sup>|빠르고 저렴함|컴퓨터에 의한 자동 채점. 정성 평가와 비슷할수록 좋은 알고리즘.|

## 실무에서 평가가 진행되는 과정의 예

저자는 이제까지 다양한 인공지능 서비스를 배포해온 만큼, 다양한 평가 진행 경험을 가지고 있는데요.
실제 실무에서 평가를 진행한 경험을 간략하게 소개하면 다음과 같습니다.

1. 학습이 완료된 모델에 테스트 데이터셋을 넣어 추론을 수행합니다.
2. 또한 동일한 테스트 데이터셋을 다른 베이스라인 모델들에 넣어 추론을 수행합니다. 이때 베이스라인 모델의 코드를 가지고 있을 수도 있지만, 경쟁사 서비스와 같이 코드가 없는 경우에는 직접 서비스에 데이터를 넣어 결과를 얻어올 수 있을 것입니다.
3. 채점 기준이 모호한 경우에는 정량평가를 수행합니다. 이때 중요한 것은 누가 어떤 예측 결과를 내놓았는지 가려서 블라인드 테스트<sup>blind test</sup>를 수행해야 한다는 것입니다. 만약 블라인드 테스트가 수행되지 않으면, 이 테스트는 객관성을 갖추지 못했다고 볼 수 있습니다.<sup>[[1]](#footnote_1)</sup>
4. 미리 잘 정의된 정량평가 기준<sup>[[2]](#footnote_2)</sup>에 따라 평가가 완료되면, 결과를 취합하여 보고합니다.

이러한 정량평가 과정은 앞서 언급하였듯이 매우 비싸고 시간이 오래 걸리기 때문에, 작은 개선사항이 있는 모델을 가지고 매번 정량평가를 수행하는 것은 매우 비효율적일 것입니다.
따라서 작은 개선사항에 대해서는 주로 정성평가를 통해 성능을 평가하고, 이후에 이것을 실제 서비스에 배포하고자 할 때 정성평가를 수행하는 것이 맞습니다.<sup>[[3]](#footnote_3)</sup>
즉, 큰 배포 주기에 따라 정성평가를 수행하고, 정량평가는 수시로 수행되겠지요.

<a name="footnote_1">[1]</a>: 사람 마음이 참 간사하기 때문에, 만약 누가 어떤 결과를 예측했는지 알고 있다면 자꾸 편향된 채점을 진행할 수 밖에 없습니다.

<a name="footnote_2">[2]</a>: 베이스라인들과의 비교 등수를 통해 상대 점수로 매기거나, 기준에 따른 배점(e.g. 상/중/하)을 통해 절대 점수로 평가할 수 있습니다.

<a name="footnote_3">[3]</a>: 서비스 배포는 신중하게 진행되어야 하기 때문에, 작은 개선사항을 반영한 모델을 바로 배포할 일은 거의 없습니다.
