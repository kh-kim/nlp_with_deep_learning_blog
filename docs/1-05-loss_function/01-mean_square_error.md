---
layout: default
title: Mean Square Errors
parent: Loss Function
nav_order: 1-05-01
---

# Loss Function

우리는 우리가 갖고 있는 모델을 통해 알 수 없는 함수 $f^*$ 를 근사하고 싶습니다.
앞선 장에서 우리는 선형 계층을 통해 모델을 구성할 수 있음을 배웠습니다.
그럼 이제 데이터를 집어 넣고 우리의 모델이 함수 $f^*$ 를 잘 근사하고 있는지 판단할 수 있어야 합니다.
어떤 방법을 통해 우리는 그것을 판단할 수 있을까요?

가장 간단한 방법은 우리의 모델에 수집한 데이터로 입력을 넣었을 때, 원하는 출력이 나오는지 확인하면 될 것입니다.
그럼 당연히 모델은 원하는 출력이 아닌 다른 값을 반환할 것입니다.
이때, 모델에서 반환한 출력은 가짜 출력이라고 볼 수 있으므로 $\hat{y}$ 라고 표현하도록 하겠습니다.

$$\begin{gathered}
\mathcal{D}=\{(x_i, y_i)\}_{i=1}^N \\
\\
\hat{y}_i=f(x_i)
\end{gathered}$$

이때, 그럼 우리는 원하는 출력 값과 모델이 반환한 출력 값을 비교해서 차이가 작을수록 좋은 모델이라고 판단할 수 있을 것입니다.
상기한 내용을 수식으로 나타내면 다음과 같습니다.

$$\begin{aligned}
\text{Loss}&=\sum_{i=1}^N{\|y_i-\hat{y}_i\|} \\
&=\sum_{i=1}^N{\|y_i-f(x_i)\|} \\
\end{aligned}$$

앞의 수식에서 모든 데이터의 입력에 대한 가짜 출력 $\hat{y}$ 과 실제 정답 $y$ 사이의 차이의 크기를 더한 것을 우리는 손실<sup>loss</sup> 값이라고 부릅니다.
즉, 손실 값이 작을수록 우리의 모델은 근사하고자 하는 함수 $f^*$ 를 잘 근사하고 있다고 판단할 수 있습니다.

여기서 우리의 모델 함수는 가중치 파라미터에 의해 동작이 정의된다고 하였습니다.
현재 우리는 선형 계층만을 배운 상태이므로 이 모델은 $W$ 와 $b$ 만을 가중치 파라미터로 갖고 있을 것입니다.<sup>[[1]](#footnote_1)</sup>
모델의 가중치 파라미터들의 집합을 이제 $\theta$ 라고 표현하도록 하겠습니다.
그럼 모델 함수는 가중치 파라미터 $\theta$ 에 의해 정의되므로, $f_\theta$ 라고 표현되어도 무방합니다.

이제 우리는 손실 값을 최소화 하는 모델을 찾으면 됩니다.
그 방법은 여러가지가 있겠지만, 가장 간단한 방법은 모델 가중치 파라미터의 값을 랜덤<sup>random</sup>하게 바꿔보는겁니다.
그럼 모델의 동작이 바뀌면서, 입력에 대한 출력 $\hat{y}$ 이 바뀌겠지요.
따라서 손실 값도 바뀌게 됩니다.
그때, 이전보다 손실 값이 더 줄어들었다면 우리는 이전보다 더 좋은 가중치 파라미터를 찾았다고 볼 수 있습니다.

그럼 이 작업을 수월하게 하기 위해서 우리는 이 작업을 함수로 표현해볼 수 있습니다.
모델의 가중치 파라미터가 바뀌면 손실 값이 바뀌게 되니까, 함수의 입력으로 가중치 파라미터를 주고 출력으로 손실 값을 반환하도록 만들어볼 수 있습니다.
이것이 바로 다음의 수식으로 표현되는 손실 함수<sup>loss function</sup>가 됩니다.

$$\begin{gathered}
\mathcal{L}(\theta)=\sum_{i=1}^N{\|y_i-f_\theta(x_i)\|}, \\
\text{where }\theta=\{W,b\}.
\end{gathered}$$

이제 우리는 손실 함수의 출력 값인 손실 값을 최소로 만드는 모델의 가중치 파라미터 $\theta$ 를 찾기만 하면 됩니다.
다만 물론 앞서 언급한 방법으로 찾는다면 아마 매우 오래 걸리겠지만요...
다음 장에서 좀 더 효율적인 방법을 배우도록 하겠습니다.

<a name="footnote_1">[1]</a>: 나중에 우리가 심층신경망과 같은 복잡한 모델을 배운 후에는 더 복잡한 모델로 바꿔치기 할 수 있습니다.

## Mean Square Errors

앞서 우리는 손실 값의 정의로 타겟 출력 $y$ 와 모델의 출력 $\hat{y}$ 와의 차이의 크기의 합이라고 하였습니다.
이때 차이의 크기를 정의하는 다양한 방법이 있습니다.

### L1

먼저 L1 노름<sup>norm</sup>을 생각해볼 수 있습니다.
이는 n차원 벡터의 각 요소들 사이의 차이에 대한 절대값들을 모두 더한 것으로, 두 벡터 사이의 L1 노름은 다음과 같이 수식으로 표현됩니다.

$$\begin{gathered}
\begin{aligned}
\|y-\hat{y}\|&=|y_1-\hat{y}_1|+\cdots+|y_n-\hat{y}_n| \\
&=\sum_{i=1}^n{
    |y_i-\hat{y}_i|
},
\end{aligned} \\
\text{where }y\in\mathbb{R}^n\text{ and }\hat{y}\in\mathbb{R}^n.
\end{gathered}$$

### L2

그리고 L2 노름이 있습니다.
이는 유클리디안 거리<sup>Euclidean distance</sup>로도 잘 알려져 있는데요.
즉, 두 점 사이의 거리를 계산하는 방법입니다.
따라서 우리는 L2 노름을 손실 함수에서 활용하게 되면, 정답과 모델 출력 사이의 거리를 최소화 한다고도 볼 수 있습니다.
L2 노름은 벡터의 각 요소들 사이의 차이에 대해 제곱을 구하여 모두 더한 것으로, 두 벡터 사이의 L2 노름은 다음과 같이 수식으로 표현됩니다.

$$\begin{gathered}
\begin{aligned}
\|y-\hat{y}\|_2&=\sqrt{(y_1-\hat{y}_1)^2+\cdots+(y_n-\hat{y}_n)^2} \\
&=\sqrt{\sum_{i=1}^n{(y_i-\hat{y}_i)^2}},
\end{aligned} \\
\text{where }y\in\mathbb{R}^n\text{ and }\hat{y}\in\mathbb{R}^n.
\end{gathered}$$

### RMSE

평균 제곱근 오차<sup>Root Mean Squared Error, RMSE</sup>는 앞서 살펴본 L2 노름과 매우 유사한 수식을 갖고 있습니다.
다만 제곱근을 구하기에 앞서, 벡터의 차원의 크기인 n으로 나누어주어 평균을 취하는 것을 볼 수 있습니다.

$$\begin{gathered}
\text{RMSE}(y,\hat{y})=\sqrt{\frac{1}{n}\sum_{i=1}^n{(y_i-\hat{y}_i)^2}}
\end{gathered}$$

### MSE

우리가 가장 많이 애용할 함수는 평균 제곱 오차<sup>Mean Squared Error, MSE</sup>입니다.
앞서 살펴본 RMSE에서 제곱근에 해당하는 R이 빠졌습니다.
즉, RMSE에 제곱을 취한 것과 같습니다.
따라서 훨씬 큰 차이 값을 반환하게 될텐데요.
상관 없습니다.
앞서 RMSE에서 상수인 n으로 나눠주었을 때도 문제가 없었듯이, 제곱은 여전히 단조증가<sup>monotonic increasing</sup> 형태를 띄므로 값들의 차이의 순서가 바뀌진 않아 문제가 되지 않습니다.
MSE의 수식은 다음과 같습니다.

$$\begin{aligned}
\text{MSE}(y,\hat{y})&=\frac{1}{n}\sum_{i=1}^n{(y_i-\hat{y}_i)^2} \\
&=\frac{1}{n}(\|y-\hat{y}\|_2)^2 \\
&=\frac{1}{n}\|y-\hat{y}\|_2^2 \\
&\propto\|y-\hat{y}\|_2^2
\end{aligned}$$

앞의 수식에서 우리는 MSE와 L2 노름간의 관계도 살펴볼 수 있습니다.
즉, MSE는 L2노름의 제곱에 상수를 곱한 값이므로, 흔히 MSE와 L2 노름의 제곱을 혼용하여 표기합니다.

만약, 우리가 MSE를 손실함수로 활용하게 된다면 다음과 같이 손실함수가 최종적으로 정의될 수 있을 것입니다.

$$\begin{aligned}
\mathcal{L}_\text{MSE}(\theta)=\frac{1}{m}\sum_{i=1}^N{
    \sum_{j=1}^m{\Big(
            y_{i,j}-f(x_{i,j})
    \Big)^2}
}
\end{aligned}$$

N개의 데이터 샘플에 대한 손실 값은, 각 샘플의 타겟 출력 값 m차원의 벡터와 모델의 출력 값 m차원의 벡터 사이의 MSE에 대한 합으로 정의됩니다.
경우에 따라 샘플의 숫자 N을 추가적으로 나누어, 샘플들의 MSE에 대한 평균을 취하기도 합니다.<sup>[[2]](#footnote_2)</sup>

<a name="footnote_2">[2]</a>: 하지만 이것은 모델의 성능에 영향이 거의 없습니다.
