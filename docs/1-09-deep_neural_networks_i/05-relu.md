---
layout: default
title: ReLU
parent: Deep Neural Networks I
nav_order: 1-09-05
---

# ReLU

$$\begin{gathered}
y=\text{ReLU}(x)=\max(0,x)
\end{gathered}$$

$$\begin{gathered}
y=\text{LeakyReLU}_\alpha(x)=\max(\alpha\cdot{x},x), \\
\text{where }0\le\alpha<1.
\end{gathered}$$
